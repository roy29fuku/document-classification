{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load CoNLL-2003 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data_dir = '../data/conll_2003/'\n",
    "    train_file_path = os.path.join(data_dir, 'eng.train')\n",
    "    test_file_path = os.path.join(data_dir, 'eng.testb')\n",
    "    train_data = load_conll2003_data(train_file_path)\n",
    "    test_data = load_conll2003_data(test_file_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def load_conll2003_data(file_path):\n",
    "    data = []\n",
    "    sentence, label = [], []\n",
    "    \n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "\n",
    "            if line:\n",
    "                word, pos, _, _ = line.split()\n",
    "                sentence.append(word)\n",
    "                label.append(pos)\n",
    "            else:\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                data.append([sentence, label])\n",
    "                sentence, label = [], []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14041"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " ['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 23624\n",
      "output_size: 45\n"
     ]
    }
   ],
   "source": [
    "UNK_TOKEN = '<UNK>'\n",
    "UNK = 0\n",
    "\n",
    "word_to_ix = {\n",
    "    UNK_TOKEN: UNK\n",
    "}\n",
    "tag_to_ix = {}\n",
    "\n",
    "for sent, tags in train_data:\n",
    "    for word, tag in zip(sent, tags):\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "            \n",
    "vocab_size = len(word_to_ix)\n",
    "output_size = len(tag_to_ix)\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('output_size: {}'.format(output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] if w in to_ix else to_ix[UNK_TOKEN] for w in seq ]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2out(lstm_out.view(len(sentence), -1))\n",
    "        outputs = self.softmax(tag_space)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "model = LSTMTagger(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, output_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ryota/.pyenv/versions/3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.596\n",
      "[1,  4000] loss: 1.873\n",
      "[1,  6000] loss: 1.556\n",
      "[1,  8000] loss: 1.719\n",
      "[1, 10000] loss: 1.688\n",
      "[1, 12000] loss: 1.607\n",
      "[1, 14000] loss: 1.670\n",
      "[2,  2000] loss: 1.554\n",
      "[2,  4000] loss: 1.361\n",
      "[2,  6000] loss: 1.213\n",
      "[2,  8000] loss: 1.408\n",
      "[2, 10000] loss: 1.446\n",
      "[2, 12000] loss: 1.393\n",
      "[2, 14000] loss: 1.459\n",
      "[3,  2000] loss: 1.369\n",
      "[3,  4000] loss: 1.205\n",
      "[3,  6000] loss: 1.079\n",
      "[3,  8000] loss: 1.271\n",
      "[3, 10000] loss: 1.311\n",
      "[3, 12000] loss: 1.271\n",
      "[3, 14000] loss: 1.333\n",
      "[4,  2000] loss: 1.253\n",
      "[4,  4000] loss: 1.104\n",
      "[4,  6000] loss: 0.980\n",
      "[4,  8000] loss: 1.172\n",
      "[4, 10000] loss: 1.213\n",
      "[4, 12000] loss: 1.183\n",
      "[4, 14000] loss: 1.240\n",
      "[5,  2000] loss: 1.167\n",
      "[5,  4000] loss: 1.029\n",
      "[5,  6000] loss: 0.909\n",
      "[5,  8000] loss: 1.100\n",
      "[5, 10000] loss: 1.134\n",
      "[5, 12000] loss: 1.114\n",
      "[5, 14000] loss: 1.172\n",
      "[6,  2000] loss: 1.100\n",
      "[6,  4000] loss: 0.968\n",
      "[6,  6000] loss: 0.854\n",
      "[6,  8000] loss: 1.040\n",
      "[6, 10000] loss: 1.073\n",
      "[6, 12000] loss: 1.060\n",
      "[6, 14000] loss: 1.116\n",
      "[7,  2000] loss: 1.049\n",
      "[7,  4000] loss: 0.916\n",
      "[7,  6000] loss: 0.809\n",
      "[7,  8000] loss: 0.989\n",
      "[7, 10000] loss: 1.023\n",
      "[7, 12000] loss: 1.011\n",
      "[7, 14000] loss: 1.065\n",
      "[8,  2000] loss: 0.996\n",
      "[8,  4000] loss: 0.871\n",
      "[8,  6000] loss: 0.768\n",
      "[8,  8000] loss: 0.948\n",
      "[8, 10000] loss: 0.982\n",
      "[8, 12000] loss: 0.969\n",
      "[8, 14000] loss: 1.023\n",
      "[9,  2000] loss: 0.956\n",
      "[9,  4000] loss: 0.834\n",
      "[9,  6000] loss: 0.731\n",
      "[9,  8000] loss: 0.912\n",
      "[9, 10000] loss: 0.948\n",
      "[9, 12000] loss: 0.932\n",
      "[9, 14000] loss: 0.989\n",
      "[10,  2000] loss: 0.925\n",
      "[10,  4000] loss: 0.803\n",
      "[10,  6000] loss: 0.707\n",
      "[10,  8000] loss: 0.882\n",
      "[10, 10000] loss: 0.916\n",
      "[10, 12000] loss: 0.901\n",
      "[10, 14000] loss: 0.956\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    running_loss = 0\n",
    "    for i, (sentence, tags) in enumerate(train_data):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        outputs = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0            \n",
    "        \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ryota/.pyenv/versions/3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',', 'NNP', 'IN', 'DT', 'NN', '.']\n",
      "tensor([[ -0.9962,  -4.0845,  -1.9076,  -1.5714, -10.8474,  -3.8860,\n",
      "         -14.7771,  -3.7973,  -6.3166,  -4.0315,  -9.0635,  -7.7966,\n",
      "          -2.2779,  -6.2082, -10.6199,  -3.7595,  -9.0452,  -5.6011,\n",
      "         -11.9054,  -4.7722, -15.0529,  -5.4295, -10.9981,  -6.9006,\n",
      "         -14.5471, -15.7445,  -9.0105,  -7.6663,  -6.9974,  -3.5209,\n",
      "          -7.2178,  -7.5900,  -9.0289,  -7.2662,  -8.9846,  -4.6342,\n",
      "          -6.4313,  -8.4732,  -7.2400,  -8.6017,  -9.2454,  -8.1779,\n",
      "          -5.8387,  -9.3872, -10.0225],\n",
      "        [ -5.3286,  -6.1853,  -6.6719,  -7.7831, -11.4943,  -4.8145,\n",
      "         -10.7168,  -0.1404,  -7.8058,  -6.7955,  -4.4008,  -9.1378,\n",
      "          -3.9012,  -5.3213,  -6.7828,  -6.2387, -14.0511,  -7.3988,\n",
      "         -17.3935,  -2.8861, -13.1953,  -7.9786,  -9.7156,  -9.4470,\n",
      "         -11.5997, -14.3455, -13.6774,  -6.2772,  -8.4087,  -5.7706,\n",
      "          -7.9856,  -6.0756,  -8.0978,  -7.2157,  -5.3873,  -6.9292,\n",
      "         -14.0573, -10.2560,  -8.7790,  -9.9274, -10.1244,  -8.8323,\n",
      "          -7.9124, -10.9959, -11.6167],\n",
      "        [ -0.8542,  -3.8467,  -2.2200,  -1.9423, -12.1503,  -4.3228,\n",
      "         -15.3251,  -2.4507,  -7.5394,  -4.6482,  -9.8401,  -9.3738,\n",
      "          -2.0547,  -6.0417, -10.7646,  -4.3082, -10.4942,  -5.9753,\n",
      "         -14.0885,  -4.5468, -16.3496,  -6.6130, -10.3529,  -8.5321,\n",
      "         -14.4301, -17.0105,  -9.9545,  -7.5107,  -7.4960,  -3.8866,\n",
      "          -7.8656,  -7.7289,  -9.4872,  -7.9396,  -9.0114,  -4.9330,\n",
      "          -9.3888,  -9.3565,  -7.8758,  -9.2832, -10.1246,  -8.7455,\n",
      "          -6.5443, -10.3249, -10.9006],\n",
      "        [ -0.8500,  -3.6398,  -2.1449,  -1.8699, -12.2118,  -4.5914,\n",
      "         -15.2306,  -2.8880,  -7.3486,  -4.4301, -10.1997,  -9.3662,\n",
      "          -2.0169,  -6.1099, -10.6843,  -4.2546, -10.2082,  -5.9225,\n",
      "         -13.8011,  -4.6003, -16.5422,  -6.6587, -10.1774,  -8.5505,\n",
      "         -14.3988, -17.0113,  -9.6216,  -7.3432,  -7.4391,  -3.8783,\n",
      "          -7.8328,  -7.7590,  -9.5322,  -7.9533,  -9.1309,  -4.9674,\n",
      "          -9.1739,  -9.2928,  -7.8511,  -9.2473, -10.1383,  -8.7593,\n",
      "          -6.4650, -10.2765, -10.8457],\n",
      "        [ -0.8831,  -3.5729,  -2.1079,  -1.8313, -12.3041,  -4.5669,\n",
      "         -15.1223,  -3.0133,  -7.3130,  -4.3195, -10.1787,  -9.2474,\n",
      "          -1.9948,  -6.0391, -10.6066,  -4.1544, -10.1137,  -5.8730,\n",
      "         -13.7273,  -4.5514, -16.4720,  -6.6550, -10.0964,  -8.4550,\n",
      "         -14.4167, -17.0305,  -9.6185,  -7.2269,  -7.3687,  -3.8289,\n",
      "          -7.7804,  -7.7036,  -9.4529,  -7.9395,  -9.0777,  -4.9850,\n",
      "          -9.1311,  -9.2456,  -7.8226,  -9.2047, -10.0985,  -8.7553,\n",
      "          -6.4217, -10.2307, -10.8100],\n",
      "        [ -1.0873,  -4.0040,  -2.0140,  -1.7669, -12.8136,  -4.4134,\n",
      "         -16.0615,  -2.1796,  -7.4698,  -4.8496,  -9.7918,  -9.6980,\n",
      "          -1.9123,  -6.4883, -10.9457,  -4.2424, -10.5596,  -6.1039,\n",
      "         -15.0228,  -4.7373, -17.2479,  -6.6877, -10.7771,  -8.6001,\n",
      "         -15.4791, -17.5353, -10.5900,  -7.8950,  -7.6425,  -3.9237,\n",
      "          -8.0550,  -7.8452,  -9.7047,  -8.0942,  -9.2425,  -5.0196,\n",
      "          -9.2643,  -9.5234,  -8.1025,  -9.5168, -10.3974,  -8.9931,\n",
      "          -6.6411, -10.5490, -11.1715],\n",
      "        [ -0.9670,  -4.4317,  -2.1180,  -1.6806, -12.8015,  -3.8872,\n",
      "         -16.4005,  -2.1650,  -8.3390,  -5.0669,  -9.8562,  -9.4626,\n",
      "          -2.2438,  -6.3237, -11.6281,  -4.2757, -11.2032,  -6.3357,\n",
      "         -15.0623,  -4.9497, -16.9027,  -6.8222, -11.4412,  -8.5516,\n",
      "         -15.9225, -18.3664, -11.1843,  -8.3130,  -7.8554,  -3.9908,\n",
      "          -8.2251,  -8.0556,  -9.9351,  -8.2548,  -9.4131,  -5.1504,\n",
      "          -9.7133,  -9.7607,  -8.2919,  -9.7147, -10.6595,  -9.2371,\n",
      "          -6.8916, -10.7578, -11.4364],\n",
      "        [ -0.9287,  -4.7594,  -3.4347,  -2.8806, -19.7016,  -3.5545,\n",
      "         -19.0795,  -1.0976, -10.5542,  -8.4760,  -8.5479, -13.9435,\n",
      "          -2.4558,  -5.1701, -13.8806,  -5.8665,  -9.7939,  -5.5289,\n",
      "         -17.9565,  -4.1351, -12.9650,  -6.9475,  -6.6956,  -9.4566,\n",
      "         -11.3753, -11.6700, -11.8093,  -8.7593,  -7.4266,  -4.0310,\n",
      "         -10.2467,  -9.9107,  -6.5296, -13.1776,  -9.7321,  -4.8837,\n",
      "         -13.5028, -11.2344,  -8.4460, -10.0039,  -7.6334,  -9.2325,\n",
      "          -9.0054, -11.7700, -11.4999],\n",
      "        [ -0.5053,  -5.1582,  -2.5413,  -1.3966, -20.1111,  -6.3320,\n",
      "         -23.3152,  -4.6911, -11.1650,  -8.4463, -14.6286, -15.6685,\n",
      "          -3.1754,  -8.6255, -17.3346,  -6.7103,  -9.9609,  -7.0805,\n",
      "         -17.6549,  -7.5594, -19.1533,  -8.3004, -10.2770, -11.3246,\n",
      "         -16.3151, -16.8401, -11.0321, -11.4766,  -9.2601,  -5.2130,\n",
      "         -11.8099, -12.5330, -10.7260, -14.2540, -14.0895,  -5.8448,\n",
      "         -11.5505, -12.4207, -10.0350, -11.7408, -10.7557, -11.2443,\n",
      "          -9.6568, -13.2796, -13.1890],\n",
      "        [ -0.8973,  -3.6582,  -2.1315,  -1.9144, -12.2446,  -4.6563,\n",
      "         -15.3407,  -2.6471,  -7.1442,  -4.5504, -10.0129,  -9.5458,\n",
      "          -1.9299,  -6.2328, -10.5981,  -4.3104, -10.1181,  -5.8956,\n",
      "         -14.0186,  -4.5710, -16.6587,  -6.5687, -10.1252,  -8.5756,\n",
      "         -14.3685, -16.7081,  -9.6227,  -7.4134,  -7.4349,  -3.8802,\n",
      "          -7.8463,  -7.7462,  -9.4741,  -7.9512,  -9.1017,  -4.8978,\n",
      "          -9.0541,  -9.2769,  -7.8274,  -9.2350, -10.0529,  -8.6935,\n",
      "          -6.4510, -10.2644, -10.8119],\n",
      "        [ -1.0273,  -4.4156,  -2.3249,  -2.2133, -12.6444,  -5.5136,\n",
      "         -17.5204,  -1.4021,  -7.3040,  -5.9769, -10.5710, -11.5745,\n",
      "          -1.9807,  -7.7478, -11.6113,  -5.3911, -10.9486,  -6.6057,\n",
      "         -16.1135,  -5.4210, -18.9180,  -6.9058, -11.2017,  -9.8998,\n",
      "         -15.4879, -17.0423, -10.3046,  -9.0236,  -8.3980,  -4.5613,\n",
      "          -8.7886,  -8.6335, -10.6200,  -8.5747, -10.1575,  -4.9741,\n",
      "          -9.4163, -10.0913,  -8.5280, -10.0833, -10.8706,  -9.1848,\n",
      "          -7.1241, -11.1521, -11.6132],\n",
      "        [ -0.0628,  -6.8986,  -5.4650,  -4.4960,  -7.6406,  -5.2524,\n",
      "         -16.5668,  -3.5530, -12.2300,  -7.4491, -13.7512, -10.9647,\n",
      "          -5.6385,  -7.0738, -14.9795,  -8.2794, -15.5224,  -8.8382,\n",
      "         -12.1018,  -7.3420, -15.7169,  -8.5550, -14.1560, -11.9692,\n",
      "         -12.7748, -19.5549,  -9.4970, -10.8253, -10.6629,  -6.6342,\n",
      "          -9.9814, -10.5654, -13.3969,  -8.9602, -11.6659,  -6.2224,\n",
      "         -12.9777, -11.6756,  -9.6168, -11.3099, -12.8112, -10.1234,\n",
      "          -8.9624, -12.3860, -12.8052]])\n",
      "(tensor([-0.9962, -0.1404, -0.8542, -0.8500, -0.8831, -1.0873, -0.9670,\n",
      "        -0.9287, -0.5053, -0.8973, -1.0273, -0.0628]), tensor([ 0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))\n",
      "tensor([ 0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "Accuracy of the network on the 10000 test images: 17 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in test_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        outputs = model(sentence_in)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
